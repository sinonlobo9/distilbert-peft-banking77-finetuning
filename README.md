# ğŸ’¬ Fine-Tuning a Large Language Model on the Banking77 Dataset
**ğŸ‘¨â€ğŸ’» Author:** Sinon Lobo  
**ğŸ“˜ Course:** Advanced NLP / LLM Fine-Tuning  
**ğŸ“… Date:** October 2025  

---

## ğŸ§  Project Overview
This project explores **parameter-efficient fine-tuning (PEFT)** using **LoRA (Low-Rank Adaptation)** on the **DistilBERT** model to perform **intent classification** on the **Banking77 dataset**.  

The primary goal was to adapt a pre-trained transformer to accurately understand **customer banking queries** such as *â€œI lost my cardâ€* or *â€œHow can I reset my PIN?â€* â€” while significantly reducing computational cost.

ğŸš€ Using LoRA, we updated **less than 1%** of DistilBERTâ€™s parameters and achieved a remarkable **94% accuracy**, demonstrating that lightweight fine-tuning methods can rival traditional full-model training in performance while being much more efficient.

---

## ğŸ¯ Project Objectives
âœ… Fine-tune a pre-trained **DistilBERT** model using **LoRA**  
âœ… Classify **77 distinct banking-related intents** from short text queries  
âœ… Leverage **PEFT** for efficient, low-resource fine-tuning  
âœ… Achieve high accuracy and F1-score with minimal parameter updates  
âœ… Build an interactive **Gradio-based web interface** for real-time inference  

---

## ğŸ“Š Dataset Overview
**Dataset:** [Banking77 (Hugging Face)](https://huggingface.co/datasets/banking77)  
**Task Type:** Intent Classification  

The **Banking77 dataset** is a benchmark dataset in conversational AI, designed for **fine-grained intent detection** in customer support scenarios.  

### ğŸ“ Dataset Details
- **Classes:** 77 user intent categories  
- **Training Samples:** ~10,000  
- **Testing Samples:** ~3,000  
- **Input:** User query text  
- **Output:** Intent label (category)  

### ğŸ§© Example Entries
| User Query | Intent Label |
|-------------|--------------|
| â€œHow do I activate my card?â€ | `activate_my_card` |
| â€œI forgot my PIN number.â€ | `forgot_pin` |
| â€œWhen will my card arrive?â€ | `card_arrival` |

Preprocessing included **tokenization**, **padding**, **truncation**, and **label encoding** â€” ensuring compatibility with the Hugging Face Transformers pipeline.

---

## ğŸ§© Model Architecture
### ğŸ§± Base Model: `distilbert-base-uncased`
DistilBERT is a **lightweight, faster version of BERT** that retains about **97%** of its performance while being:
- âš¡ 60% faster in inference
- ğŸ’¾ 40% smaller in size
- ğŸ§  Ideal for limited compute environments  

It uses **6 transformer encoder layers** (compared to 12 in BERT) and was trained using **knowledge distillation** to compress language understanding into a smaller model.

---

### âš™ï¸ Why DistilBERT + LoRA?
| Feature | Benefit |
|----------|----------|
| Lightweight Transformer | Perfect for small-scale fine-tuning |
| LoRA Adaptation | Updates <1% of weights efficiently |
| PEFT Library | Simplifies LoRA integration and management |
| Faster Convergence | Less training time with similar accuracy |

---

### ğŸ§® LoRA Configuration
LoRA introduces trainable low-rank matrices (A, B) into attention layers, replacing full weight updates with efficient transformations.

```python
from peft import LoraConfig, TaskType

peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    r=8,                 # Rank of low-rank matrices
    lora_alpha=16,       # Scaling factor
    lora_dropout=0.1,    # Dropout rate
)
